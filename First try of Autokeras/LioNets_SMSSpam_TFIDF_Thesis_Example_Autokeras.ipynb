{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rx3XWSIfHrph"
   },
   "outputs": [],
   "source": [
    "!pip install Keras==2.4.3\n",
    "!pip install tensorflow==2.3.0\n",
    "!pip install lime==0.1.1.32\n",
    "!pip install nltk==3.2.4\n",
    "!pip install autokeras\n",
    "!pip install keras-tuner==1.0.2rc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j29msUC1Hrpt"
   },
   "source": [
    "# LioNets: SMSSpam with TFIDF and Neural Networks -> Classification Task\n",
    "\n",
    "In this notebook, we present how LioNets can be applied in predictive models using textual data with TFIDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0lvvegHHrpv"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7NY_v7uHrp1"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import Input, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import Ridge, SGDRegressor, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score, balanced_accuracy_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from lime.lime_tabular import LimeTabularExplainer #Maybe wont use\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',400)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from load_dataset import Load_Dataset\n",
    "from LioNets import LioNet\n",
    "from evaluation import Evaluation\n",
    "import autokeras as ak\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AMF55LKHrp4",
    "outputId": "2f429630-15d3-4df2-9445-8a0b3eb6db14"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dOQaZuUHrp8"
   },
   "source": [
    "Load Spam SMS Dataset and split it 80-20 to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLAGBFtFHrp8"
   },
   "outputs": [],
   "source": [
    "X, y, class_names = Load_Dataset.load_smsspam()\n",
    "X_train, X_valid, y_train, y_valid =  train_test_split(X,y,test_size=0.2, stratify = y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXLxTuwoHrp_"
   },
   "source": [
    "Load few data to use them as unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zR0RTs7Hrp_"
   },
   "outputs": [],
   "source": [
    "#X_unsup = Load_Dataset.load_unsupervised_data(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnaT7-W3HrqC"
   },
   "source": [
    "Setting the TFIDF vectorizer, with 1K vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_xd_Mg5HrqD"
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer='word',max_features=1000)\n",
    "vec.fit(X_train)\n",
    "x_train = vec.transform(X_train).A\n",
    "x_valid = vec.transform(X_valid).A\n",
    "#x_unsup = vec.transform(X_unsup)\n",
    "#x_train #input of predictor and encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsjVt3M4HrqG"
   },
   "source": [
    "Defining the input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw0tQIpvHrqH",
    "outputId": "547c9ae4-f2a9-450e-88f7-c49ef0cbbe58"
   },
   "outputs": [],
   "source": [
    "input_dim = len(vec.get_feature_names())\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQDc6AY1HrqK"
   },
   "source": [
    "Changing the format of the output from 0->0.1 and 1->0.9 in order to help the neural network with the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGaeMiHFHrqK"
   },
   "outputs": [],
   "source": [
    "train_y = [0.1 if i <=0.5 else 0.9 for i in y_train]\n",
    "valid_y = [0.1 if i <=0.5 else 0.9 for i in y_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVLOKMNNHrqN"
   },
   "source": [
    "We are setting the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRwfWsDEHrqO"
   },
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\"SMS_Predictor2020.hdf5\", monitor=\"val_loss\", verbose=2,save_best_only=True, mode=\"auto\")\n",
    "main_input = Input(shape=(input_dim,), dtype='float32', name='main_input')\n",
    "x = Reshape((1,input_dim))(main_input)\n",
    "x = LSTM(1000,activation='tanh')(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Dense(500,activation='tanh')(x)\n",
    "output_lay = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[main_input], outputs=[output_lay])\n",
    "model.compile(optimizer=\"adam\",loss=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziWFzIaUHrqQ"
   },
   "source": [
    "We train the model (we do not use unsupervised data yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYPVEO0gHrqQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, train_y, epochs=150, batch_size=64, shuffle=True, validation_data=(x_valid, valid_y), verbose=2, callbacks=[check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gp-5IoGHrqS"
   },
   "source": [
    "We load the best model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d7cHqKmHrqS"
   },
   "outputs": [],
   "source": [
    "weights_file = 'weights/SMS_Predictor.hdf5' # choose the best checkpoint few features\n",
    "model.load_weights(weights_file) # load it\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_predictor = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAVqgvpfHrqU"
   },
   "source": [
    "We are testing the performance of the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqBpCyrHHrqV",
    "outputId": "23b769ff-fd0d-4b38-bf1e-9b93987cab0d"
   },
   "outputs": [],
   "source": [
    "temp_pred = model.predict(x_train)\n",
    "predictions = [0 if i[0] <=0.5 else 1 for i in temp_pred]\n",
    "print('Train:',f1_score(y_train,predictions, average='macro'),f1_score(y_train,predictions, average='weighted'),\n",
    "      balanced_accuracy_score(y_train,predictions),accuracy_score(y_train,predictions))\n",
    "\n",
    "temp_pred = model.predict(x_valid)\n",
    "predictions = [0 if i[0] <=0.5 else 1 for i in temp_pred]\n",
    "print('Test:',f1_score(y_valid,predictions, average='macro'),f1_score(y_valid,predictions, average='weighted'),\n",
    "      balanced_accuracy_score(y_valid,predictions), accuracy_score(y_valid,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqVZMAgqHrqX"
   },
   "source": [
    "We extract the encoder from the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0JpyeANHrqX"
   },
   "outputs": [],
   "source": [
    "encoder = Model(inputs=model.input, outputs=[model.layers[-2].output])\n",
    "encoder.trainable = False\n",
    "encoder.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmKCzu1QHrqZ"
   },
   "outputs": [],
   "source": [
    "output_layer = model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkFK62HkHrqa"
   },
   "outputs": [],
   "source": [
    "output_layer_weights = output_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2wCygb_Hrqc"
   },
   "source": [
    "We encode our data, in order to build the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amnY9TwrHrqc"
   },
   "outputs": [],
   "source": [
    "encoded_x_train = encoder.predict(x_train) #outputs of encoder / inputs of decoder\n",
    "encoded_x_valid = encoder.predict(x_valid)\n",
    "#encoded_x_unsup = encoder.predict(x_unsup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder - Autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(encoded_x_train)\n",
    "encoded_x_train_scaled = scaler.transform(encoded_x_train)\n",
    "\n",
    "autodecoder = ak.StructuredDataRegressor(max_trials=10, loss='binary_crossentropy')\n",
    "autodecoder.fit(encoded_x_train_scaled, x_train)\n",
    "\n",
    "autodecoder = autodecoder.export_model()\n",
    "autodecoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_autodecoder = autodecoder.predict(encoded_x_train_scaled)\n",
    "outputs_autodecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autodecoder.evaluate(encoded_x_train_scaled, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_evaluation = autodecoder.predict(encoded_x_train_scaled[:20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_evaluation_threshold = []\n",
    "for r_m in instances_evaluation:\n",
    "    a_t = [o if o > 0.045 else 0 for o in r_m]\n",
    "    instances_evaluation_threshold.append(a_t)\n",
    "inversed_decoded = vec.inverse_transform(instances_evaluation_threshold)\n",
    "inversed_original = vec.inverse_transform(x_train[:20])\n",
    "for i in range(len(inversed_original)):\n",
    "    print('Original:',' '.join(sorted(inversed_original[i])))\n",
    "    print(' Decoded:',' '.join(sorted(inversed_decoded[i])))\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_evaluation = autodecoder.predict(encoded_x_valid[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_evaluation_threshold = []\n",
    "for r_m in instances_evaluation:\n",
    "    a_t = [o if o > 0.045 else 0 for o in r_m]\n",
    "    instances_evaluation_threshold.append(a_t)\n",
    "inversed_decoded = vec.inverse_transform(instances_evaluation_threshold)\n",
    "inversed_original = vec.inverse_transform(x_valid[:20])\n",
    "for i in range(len(inversed_original)):\n",
    "    print('Original:',' '.join(sorted(inversed_original[i])))\n",
    "    print(' Decoded:',' '.join(sorted(inversed_decoded[i])))\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "nGgxuaeMHrqe"
   },
   "source": [
    "We are building now the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F9LkexDHrqe"
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysit8Wh6Hrqg",
    "outputId": "da6fe822-c3d6-42ef-aa2f-0d5d11d8439a"
   },
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(encoded_x_train[0].shape))\n",
    "\n",
    "x = Reshape((1,len(encoded_x_train[0])))(encoded_input)\n",
    "x = LSTM(600, activation='tanh')(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = Dense(800, activation='tanh')(x)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "decoder = Model(encoded_input,decoded)\n",
    "decoder.compile(optimizer=\"Adam\",loss=['binary_crossentropy'],metrics=[rmse,'mae'])\n",
    "\n",
    "checkpoint_name = 'SMS_Decoder.hdf5' #or:'SMS_TFIDF_Decoder.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 2, save_best_only = True, mode ='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6a--00lHrqh"
   },
   "source": [
    "We train the decoder, using some unsupervised data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKTPah-HHrqh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decoder.fit(np.concatenate((encoded_x_train,encoded_x_unsup[12000:13000])), np.concatenate((x_train.toarray(),x_unsup[12000:13000].toarray())), epochs=250, batch_size=250, shuffle=True, validation_data=(encoded_x_valid,x_valid), verbose=2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXZDkjlWHrqi"
   },
   "source": [
    "We load the best decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsgIVvrXHrqj"
   },
   "outputs": [],
   "source": [
    "weights_file = 'weights/SMS_Decoder.hdf5' # choose the best checkpoint few features\n",
    "decoder.load_weights(weights_file) # load it\n",
    "decoder.compile(optimizer=\"Adam\",loss=['binary_crossentropy'],metrics=[rmse,'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_decoder = decoder.predict(encoded_x_train)\n",
    "outputs_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHih4S0IHrqk",
    "outputId": "07475fd1-5dfa-419f-8014-4f867ef3151c"
   },
   "outputs": [],
   "source": [
    "decoder.evaluate(encoded_x_train,x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcKpgkH-Hrql",
    "outputId": "aed0322b-9e92-4095-8a7a-fe5ff3bb94b6"
   },
   "outputs": [],
   "source": [
    "decoder.evaluate(encoded_x_valid,x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlPkiWHIHrqn"
   },
   "source": [
    "We evaluate some decoded instances manually (The first 10 sentences from the train and test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g15DGgubHrqn"
   },
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u25X9hroHrqn"
   },
   "outputs": [],
   "source": [
    "instances_evaluation = decoder.predict(encoded_x_train[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRa9uXbqHrqo"
   },
   "source": [
    "We will use a threshold to remove close to zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSNmRNlwHrqo",
    "outputId": "ef79b0cd-5ea6-4990-9048-7ac836b04fce"
   },
   "outputs": [],
   "source": [
    "instances_evaluation_threshold = []\n",
    "for r_m in instances_evaluation:\n",
    "    a_t = [o if o > 0.045 else 0 for o in r_m]\n",
    "    instances_evaluation_threshold.append(a_t)\n",
    "inversed_decoded = vec.inverse_transform(instances_evaluation_threshold)\n",
    "inversed_original = vec.inverse_transform(x_train[:5])\n",
    "for i in range(len(inversed_original)):\n",
    "    print('Original:',' '.join(sorted(inversed_original[i])))\n",
    "    print(' Decoded:',' '.join(sorted(inversed_decoded[i])))\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVWSeUieHrqq"
   },
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxOz3gpMHrqq"
   },
   "outputs": [],
   "source": [
    "instances_evaluation = decoder.predict(encoded_x_valid[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgC8p3GWHrqr"
   },
   "source": [
    "We will use a threshold to remove close to zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCpri2hJHrqr",
    "outputId": "2ebf53d3-6da8-47bc-c2b8-1d5c9a18b5ce"
   },
   "outputs": [],
   "source": [
    "instances_evaluation_threshold = []\n",
    "for r_m in instances_evaluation:\n",
    "    a_t = [o if o > 0.045 else 0 for o in r_m]\n",
    "    instances_evaluation_threshold.append(a_t)\n",
    "inversed_decoded = vec.inverse_transform(instances_evaluation_threshold)\n",
    "inversed_original = vec.inverse_transform(x_valid[:5])\n",
    "for i in range(len(inversed_original)):\n",
    "    print('Original:',' '.join(sorted(inversed_original[i])))\n",
    "    print(' Decoded:',' '.join(sorted(inversed_decoded[i])))\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEDgNLcHHrqt"
   },
   "source": [
    "## LioNets Experiments\n",
    "Having everything setted up, we are now ready to try our methodology. We first initialize LioNets. LioNets requires a predictor (the classifier itself), an encoder (extracted from the predictor), a decoder, as well as some data (for best results the training data, in order to push the neighbourhood generation through known distribution for the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_KqgBFIHrqt"
   },
   "outputs": [],
   "source": [
    "lionet = LioNet(model, decoder, encoder, x_train, decoder_lower_threshold=0.045, double_detector=True)\n",
    "\n",
    "#lionet with autodecoder\n",
    "autolionet = LioNet(model, autodecoder, encoder, x_train, decoder_lower_threshold=0.045, double_detector=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1q_LcCcHrqu"
   },
   "source": [
    "temp_instance.A[0] for a TFIDF array to be an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "KxTdDfHsHrqu"
   },
   "source": [
    "## LioNets Experiments\n",
    "Having everything setted up, we are now ready to try our methodology, Gradient x Input and LIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhSqyxNbHrqu",
    "outputId": "f3b96a2a-6994-4d70-bce2-9125f5018509"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "#random.seed(2000)\n",
    "random.seed(7777)\n",
    "train = np.array(random.sample(X_train,200))#200\n",
    "valid = np.array(random.sample(X_valid,200))\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKgd9txkHrqv"
   },
   "source": [
    "Let's calculate the fidelity of Lime and LioNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nX6OsyiHrqv"
   },
   "outputs": [],
   "source": [
    "split_expression = lambda s: re.split(r'\\W+', s)\n",
    "explainer = LimeTextExplainer(class_names=class_names, split_expression=split_expression)\n",
    "def lime_predict(text):\n",
    "    texts = vec.transform(text)\n",
    "    a = model.predict(texts)\n",
    "    b = 1 - a \n",
    "    return np.column_stack((b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9mIpcHjHrqw"
   },
   "outputs": [],
   "source": [
    "def lime(text):\n",
    "    explanation = explainer.explain_instance(text_instance=text, classifier_fn=lime_predict)\n",
    "    weights = OrderedDict(explanation.as_list())\n",
    "    lime_w = dict(zip(list(weights.keys()), list(weights.values())))\n",
    "    weights = []\n",
    "    keys = lime_w.keys()\n",
    "    for f in vec.get_feature_names():\n",
    "        if f in keys:\n",
    "            weights.append(lime_w[f])\n",
    "        else:\n",
    "            weights.append(0)\n",
    "    return np.array([weights]) #This is because lime interprets class with label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVZQy4t3Hrqx"
   },
   "outputs": [],
   "source": [
    "def fi_lime(text):\n",
    "    explanation = explainer.explain_instance(text_instance=text, classifier_fn=lime_predict)\n",
    "    local_pred = explanation.local_pred[0]\n",
    "    return local_pred #This is because lime interprets class with label 1\n",
    "def fi_lionets(text):\n",
    "    t_text = vec.transform(np.array([text]))[0].A[0]\n",
    "    weights, res, loc_res = lionet.explain_instance(t_text,2000)\n",
    "    return loc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUeicRCFHrqy"
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluation(model.predict,None,vec.transform,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51uNHZfPHrqz",
    "outputId": "0d75af23-a2a0-483b-deae-4387e33ea672"
   },
   "outputs": [],
   "source": [
    "fidelity = evaluator.fidelity(train, [fi_lime, fi_lionets], class_n=0)\n",
    "print('Lime fidelity:', fidelity[0][0])\n",
    "print('LioNets fidelity:', fidelity[1][0])\n",
    "fidelity = evaluator.fidelity(valid, [fi_lime, fi_lionets], class_n=0)\n",
    "print('Lime fidelity:', fidelity[0][0])\n",
    "print('LioNets fidelity:', fidelity[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJbsQe1ZHrq1"
   },
   "source": [
    "Let's calculate non zero weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQpsvqmQHrq1"
   },
   "outputs": [],
   "source": [
    "Xs = iutils.to_list(model.outputs)\n",
    "softmax_found = False\n",
    "ret = []\n",
    "for x in Xs:\n",
    "    layer, node_index, tensor_index = x._keras_history\n",
    "    if checks.contains_activation(layer, activation=\"sigmoid\"):\n",
    "        softmax_found = True\n",
    "        if isinstance(layer, keras.layers.Activation):\n",
    "            ret.append(layer.get_input_at(node_index))\n",
    "        else:\n",
    "            layer_wo_act = innvestigate.utils.keras.graph.copy_layer_wo_activation(layer)\n",
    "            ret.append(layer_wo_act(layer.get_input_at(node_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0I5FBWeHrq2",
    "outputId": "6e0d2bfd-20b3-4fb5-a06e-524bc16aa629"
   },
   "outputs": [],
   "source": [
    "embedding_encoder = Sequential()\n",
    "for i in range(0,2):\n",
    "    print(model.layers[i])\n",
    "    embedding_encoder.add(model.layers[i])\n",
    "embedding_encoder.trainable = False\n",
    "embedding_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnfeFM6yHrq3"
   },
   "outputs": [],
   "source": [
    "model2 = Model(inputs=model.input, outputs=ret)\n",
    "model2.trainable = False\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOrLVza4Hrq5"
   },
   "outputs": [],
   "source": [
    "analyzer = innvestigate.create_analyzer('input_t_gradient',model2)\n",
    "analyzerLRP = innvestigate.create_analyzer('lrp.epsilon',model2)\n",
    "def LRP_analyzer(X_t):\n",
    "    ooo = analyzerLRP.analyze(X_t)[0]\n",
    "    ooo = ooo*X_t #only on lrp\n",
    "    return [ooo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UNHWUQBHrq6"
   },
   "outputs": [],
   "source": [
    "feature_names = vec.get_feature_names()\n",
    "def fi_GxI(text):\n",
    "    t_text = vec.transform(np.array([text]))[0].A[0]\n",
    "    ooo = analyzer.analyze(np.array([t_text]))[0]\n",
    "    return [ooo][0]\n",
    "def fi_LRP(text):\n",
    "    t_text = vec.transform(np.array([text]))[0].A[0]\n",
    "    ooo = analyzerLRP.analyze(np.array([t_text]))[0]\n",
    "    ooo = ooo*t_text #only on lrp\n",
    "    return [ooo][0]\n",
    "def fi_lime(text):\n",
    "    return lime(text)[0] #This is because lime interprets class with label 1\n",
    "lionet = LioNet(model, decoder, encoder, x_train, decoder_lower_threshold=0.045, double_detector=True)\n",
    "def fi_lionets(text):\n",
    "    t_text = vec.transform(np.array([text]))[0].A[0]\n",
    "    weights, res, loc_res = lionet.explain_instance(t_text,2000)\n",
    "    nonzero = np.array([o if o > 0 else 0 for o in t_text])\n",
    "    weights=weights*nonzero\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwDCsaVoHrq7",
    "outputId": "ee536db0-3396-4c73-8fce-eb205960c378"
   },
   "outputs": [],
   "source": [
    "non_zero = evaluator.non_zero_weights(train, [fi_GxI, fi_LRP, fi_lime, fi_lionets])\n",
    "print('GxI Non Zero:', non_zero[0][0])\n",
    "print('LRP Non Zero:', non_zero[1][0])\n",
    "print('Lime Non Zero:', non_zero[2][0])\n",
    "print('LioNets Non Zero:', non_zero[3][0])\n",
    "non_zero = evaluator.non_zero_weights(valid, [fi_GxI, fi_LRP, fi_lime, fi_lionets])\n",
    "print('GxI Non Zero:', non_zero[0][0])\n",
    "print('LRP Non Zero:', non_zero[1][0])\n",
    "print('Lime Non Zero:', non_zero[2][0])\n",
    "print('LioNets Non Zero:', non_zero[3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFb21gKIHrq8"
   },
   "source": [
    "Let's calculate robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1wVFuLJHrq8",
    "outputId": "025ce66d-7705-47ac-bf7e-342f5b267435"
   },
   "outputs": [],
   "source": [
    "robustness = evaluator.robustness(train,[fi_lime, fi_GxI, fi_LRP, fi_lionets], vec.get_feature_names())\n",
    "print('GxI Robustness:', robustness[0])\n",
    "print('LRP Robustness:', robustness[1])\n",
    "print('Lime Robustness:', robustness[2])\n",
    "print('LioNets Robustness:', robustness[3])\n",
    "robustness = evaluator.robustness(valid,[fi_lime, fi_GxI, fi_LRP, fi_lionets], vec.get_feature_names())\n",
    "print('GxI Robustness:', robustness[0])\n",
    "print('LRP Robustness:', robustness[1])\n",
    "print('Lime Robustness:', robustness[2])\n",
    "print('LioNets Robustness:', robustness[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPvtD8MEHrq9"
   },
   "source": [
    "Let's calculate Altruist Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afnrkkEkHrq9"
   },
   "outputs": [],
   "source": [
    "def fi_GxI(X_t, prediction, model):\n",
    "    ooo = analyzer.analyze(np.array([X_t]))[0]\n",
    "    return [ooo][0]\n",
    "def fi_LRP(X_t, prediction, model):\n",
    "    ooo = analyzerLRP.analyze(np.array([X_t]))[0]\n",
    "    ooo = ooo*X_t #only on lrp\n",
    "    return [ooo][0]\n",
    "global ccount\n",
    "ccount = 0\n",
    "def fi_lime(t, prediction, model):\n",
    "    global ccount\n",
    "    expl = lime(train[ccount])[0]\n",
    "    ccount = ccount+1\n",
    "    return expl\n",
    "def fi_lionets(X_t, prediction, model):\n",
    "    weights, res, loc_red = lionet.explain_instance(X_t,2000)\n",
    "    nonzero = np.array([o if o > 0 else 0 for o in X_t])\n",
    "    #weights=weights*X_t\n",
    "    #weights=weights*nonzero\n",
    "    weights=weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cfoXNeBHrq-",
    "outputId": "c73f36e9-5ff9-45d9-de94-d3f49eb5a881"
   },
   "outputs": [],
   "source": [
    "from altruist import Altruist\n",
    "print(\"*Please let it run, it will take time probably*\")\n",
    "fi_names = {fi_GxI:'GxI',fi_LRP:'LRP',fi_lime:'Lime',fi_lionets:'LioNets'}\n",
    "fis = [fi_GxI, fi_LRP, fi_lime,fi_lionets]\n",
    "fis_scores = []\n",
    "for i in fis:\n",
    "    fis_scores.append([])\n",
    "count = 0\n",
    "feature_names \n",
    "X_t = vec.transform(train)\n",
    "for instance in X_t:\n",
    "    if (count + 1) % 100 == 0:\n",
    "        print(count+1,\"/\",len(train),\"..\",end=\", \")\n",
    "    #print(len(instance))\n",
    "    count = count + 1\n",
    "    altruistino = Altruist(model, X_t.toarray(), fis, feature_names, None, True)\n",
    "    untruthful_features = altruistino.find_untruthful_features(instance.toarray()[0])\n",
    "    for i in range(len(untruthful_features[0])):\n",
    "        fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "count = 0\n",
    "print()\n",
    "for fis_score in fis_scores:\n",
    "    fi = fis[count]\n",
    "    count = count + 1\n",
    "    print(fi_names[fi],np.array(fis_score).mean())\n",
    "fi_matrix = np.array(fis_scores)\n",
    "count = 0\n",
    "fi_all = []\n",
    "for instance in X_t:\n",
    "    fi_all.append(fi_matrix[:,count].min())\n",
    "    count = count + 1\n",
    "print(\"Altogether:\",np.array(fi_all).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sMOmb_6Hrq_"
   },
   "outputs": [],
   "source": [
    "def fi_GxI(X_t, prediction, model):\n",
    "    ooo = analyzer.analyze(np.array([X_t]))[0]\n",
    "    return [ooo][0]\n",
    "def fi_LRP(X_t, prediction, model):\n",
    "    ooo = analyzerLRP.analyze(np.array([X_t]))[0]\n",
    "    ooo = ooo*X_t #only on lrp\n",
    "    return [ooo][0]\n",
    "global ccount\n",
    "ccount = 0\n",
    "def fi_lime(t, prediction, model):\n",
    "    global ccount\n",
    "    expl = lime(valid[ccount])[0]\n",
    "    ccount = ccount+1\n",
    "    return expl\n",
    "def fi_lionets(X_t, prediction, model):\n",
    "    weights, res, loc_red = lionet.explain_instance(X_t,2000)\n",
    "    nonzero = np.array([o if o > 0 else 0 for o in X_t])\n",
    "    #weights=weights*X_t\n",
    "    #weights=weights*nonzero\n",
    "    weights=weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA1AEbsFHrrA"
   },
   "outputs": [],
   "source": [
    "from altruist import Altruist\n",
    "print(\"*Please let it run, it will take time probably*\")\n",
    "fi_names = {fi_GxI:'GxI',fi_LRP:'LRP',fi_lime:'Lime',fi_lionets:'LioNets'}\n",
    "fis = [fi_GxI, fi_LRP, fi_lime,fi_lionets]\n",
    "fis_scores = []\n",
    "for i in fis:\n",
    "    fis_scores.append([])\n",
    "count = 0\n",
    "feature_names \n",
    "X_t = vec.transform(valid)\n",
    "for instance in X_t:\n",
    "    if (count + 1) % 100 == 0:\n",
    "        print(count+1,\"/\",len(valid),\"..\",end=\", \")\n",
    "    #print(len(instance))\n",
    "    count = count + 1\n",
    "    altruistino = Altruist(model, X_t.toarray(), fis, feature_names, None, True)\n",
    "    untruthful_features = altruistino.find_untruthful_features(instance.toarray()[0])\n",
    "    for i in range(len(untruthful_features[0])):\n",
    "        fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "count = 0\n",
    "print()\n",
    "for fis_score in fis_scores:\n",
    "    fi = fis[count]\n",
    "    count = count + 1\n",
    "    print(fi_names[fi],np.array(fis_score).mean())\n",
    "fi_matrix = np.array(fis_scores)\n",
    "count = 0\n",
    "fi_all = []\n",
    "for instance in X_t:\n",
    "    fi_all.append(fi_matrix[:,count].min())\n",
    "    count = count + 1\n",
    "print(\"Altogether:\",np.array(fi_all).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYexO8ITHrrB"
   },
   "source": [
    "Let's calculate TI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9TrppuyHrrB"
   },
   "outputs": [],
   "source": [
    "evaluatorGxI = Evaluation(model.predict,analyzer.analyze,vec.transform,True)\n",
    "evaluatorLRP = Evaluation(model.predict,LRP_analyzer,vec.transform,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmiGvISpHrrC"
   },
   "source": [
    "Gradient x Input Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLIrY9cNHrrC",
    "outputId": "a97bce11-976e-460f-e403-154ecc70bfd2"
   },
   "outputs": [],
   "source": [
    "e_results = evaluatorGxI.truthful_influence(train,fidelity=False)\n",
    "print(\"Train TI1:\",e_results[1],\" NonZero:\",e_results[0])\n",
    "e_results = evaluatorGxI.truthful_influence(valid,fidelity=False)\n",
    "print(\"Valid TI1:\",e_results[1],\" NonZero:\",e_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iMe75lqHrrD"
   },
   "source": [
    "LRP Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lO_m9S2HrrD",
    "outputId": "1047d26a-c5fc-409b-e1d8-457d6ef40065"
   },
   "outputs": [],
   "source": [
    "e_results = evaluatorLRP.truthful_influence(train,fidelity=False)\n",
    "print(\"Train TI1:\",e_results[1],\" NonZero:\",e_results[0])\n",
    "e_results = evaluatorLRP.truthful_influence(valid,fidelity=False)\n",
    "print(\"Valid TI1:\",e_results[1],\" NonZero:\",e_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbqMwPp4HrrE"
   },
   "source": [
    "Lime Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Az5M7ZbDHrrE"
   },
   "outputs": [],
   "source": [
    "evaluatorLIME = Evaluation(model.predict,lime,vec.transform,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc8w6zERHrrF",
    "outputId": "623271a5-d863-43aa-e53b-13518a0eb40b"
   },
   "outputs": [],
   "source": [
    "e_results = evaluatorLIME.truthful_influence(train,fidelity=False)\n",
    "print(\"Train TI1:\",e_results[1],\" NonZero:\",e_results[0])\n",
    "e_results = evaluatorLIME.truthful_influence(valid,fidelity=False)\n",
    "print(\"Valid TI1:\",e_results[1],\" NonZero:\",e_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfBS7mxJHrrG"
   },
   "source": [
    "LioNets Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tOOiM0AHrrG"
   },
   "outputs": [],
   "source": [
    "def lionets_eval(text):\n",
    "    weights, res, loc_red = lionet.explain_instance(text[0],2000)\n",
    "    nonzero = np.array([o if o > 0 else 0 for o in text[0]])\n",
    "    weights=weights*nonzero\n",
    "    return [weights] #This is because lime interprets class with label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfk9O_P5HrrH"
   },
   "outputs": [],
   "source": [
    "evaluatorLioNets = Evaluation(model.predict,lionets_eval,vec.transform,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1WWqGoGHrrI",
    "outputId": "06c91b9a-79d4-4139-876b-4946728bcaf1"
   },
   "outputs": [],
   "source": [
    "e_results = evaluatorLioNets.truthful_influence(train,fidelity=False)\n",
    "print(\"Train TI1:\",e_results[1],\" NonZero:\",e_results[0])\n",
    "e_results = evaluatorLioNets.truthful_influence(valid,fidelity=False)\n",
    "print(\"Valid TI1:\",e_results[1],\" NonZero:\",e_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQEeOppvHrrJ",
    "outputId": "fe7b08d5-734b-44c8-860c-c7e6e93c6c46",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"*Please let it run, it will take time probably*\")\n",
    "#for nne in [200,500,700,1000,1500,2000]:#,2000,2200,3000,4000,5000]:\n",
    "for nne in [2000,2200,3000,4000,5000]:#,2000,2200,3000,4000,5000]:\n",
    "    for tdc in [0,0.00001,0.01,0.02, 0.022, 0.035, 0.04, 0.045, 0.05, 0.07, 0.1, 0.15, 0.2, 0.22, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "        print(nne,tdc)\n",
    "        feature_names = vec.get_feature_names()\n",
    "        lionet = LioNet(model, decoder, encoder, x_train, decoder_lower_threshold=tdc, double_detector=True)\n",
    "        def fi_lionets(X_t, prediction, model):\n",
    "            weights, res, loc_red = lionet.explain_instance(X_t,nne)\n",
    "            nonzero = np.array([o if o > 0 else 0 for o in X_t])\n",
    "            #weights=weights*X_t\n",
    "            #weights=weights*nonzero\n",
    "            weights=weights\n",
    "            return weights\n",
    "        from altruist import Altruist\n",
    "        fi_names = {fi_GxI:'GxI',fi_LRP:'LRP',fi_lime:'Lime',fi_lionets:'LioNets'}\n",
    "        fis = [fi_lionets]#fi_lionets\n",
    "        fis_scores = []\n",
    "        for i in fis:\n",
    "            fis_scores.append([])\n",
    "        count = 0\n",
    "        feature_names \n",
    "        X_t = vec.transform(train)\n",
    "        for instance in X_t:\n",
    "            if (count + 1) % 100 == 0:\n",
    "                print(count+1,\"/\",len(train),\"..\",end=\", \")\n",
    "            #print(len(instance))\n",
    "            count = count + 1\n",
    "            altruistino = Altruist(model, X_t.toarray(), fis, feature_names, None, True)\n",
    "            untruthful_features = altruistino.find_untruthful_features(instance.toarray()[0])\n",
    "            for i in range(len(untruthful_features[0])):\n",
    "                fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "        print()\n",
    "        count = 0\n",
    "        for fis_score in fis_scores:\n",
    "            fi = fis[count]\n",
    "            count = count + 1\n",
    "            print(fi_names[fi],np.array(fis_score).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM-iiMQbHrrK"
   },
   "source": [
    "## Qualitative:\n",
    "Let's see an example manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv2s5JHlHrrK"
   },
   "outputs": [],
   "source": [
    "temp_instance = x_train[9].copy()\n",
    "transparent_model = Ridge(alpha=0.0001,fit_intercept=True,random_state=0)\n",
    "weights, real_prediction, local_prediction = lionet.explain_instance(temp_instance[0:],2000, transparent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TI-Bu5VHrrL",
    "outputId": "cdb502c7-852e-4b08-8622-3704ffccdda0"
   },
   "outputs": [],
   "source": [
    "str('Sentence: \"' + X_train[9] + '\"   Class: ' + str(train_y[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dahn2aRMHrrM",
    "outputId": "c28a6015-f74e-4d47-f0ba-7f06c86fe74e"
   },
   "outputs": [],
   "source": [
    "weights, real_prediction, local_prediction = lionet.explain_instance(x_train[9][0:], 2000, transparent_model)\n",
    "print(\"Real prediction:\",real_prediction,\", Local prediction:\",local_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOh0jDBqHrrN",
    "outputId": "bba821cc-81b0-4a60-d728-b102d4e4e18c"
   },
   "outputs": [],
   "source": [
    "model_weights = pd.DataFrame({\"Features\": list(vec.get_feature_names()), \n",
    "                              \"Features' Weights\": list(weights*x_train[9][0:])})\n",
    "model_weights = model_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "model_weights = model_weights[(model_weights[\"Features' Weights\"] != 0)]    \n",
    "#model_weights, lime_predict([text])[0][1], rd.predict(texts)[0], weights\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=model_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qd2qyLvaHrrP",
    "outputId": "5a41c413-8d05-4dd2-f323-69871a6c0a21",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(weights),np.argmin(weights),np.max(weights),np.min(weights),vec.get_feature_names()[np.argmax(weights)],vec.get_feature_names()[np.argmin(weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npAqZ_jZHrrQ",
    "outputId": "cb3d992b-8f03-4691-c769-a761b05ac518"
   },
   "outputs": [],
   "source": [
    "lime_predict(['this congrat treat pend am not on mail for day wil mail onc thru respect mother at home check mail'])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raPYn3LgHrrR",
    "outputId": "594dfc1a-b7c0-4a5e-e570-b45b24a3cf5b"
   },
   "outputs": [],
   "source": [
    "counter_weights = []\n",
    "counter_features = []\n",
    "for i in range(len(weights)):\n",
    "    if weights[i]!=0:\n",
    "        if vec.get_feature_names()[i] not in X_train[9]:\n",
    "            counter_weights.append(weights[i])\n",
    "            counter_features.append(vec.get_feature_names()[i])\n",
    "co_weights = pd.DataFrame({\"Counter Features\": list(counter_features), \n",
    "                                  \"Features' Weights\": list(counter_weights)})\n",
    "co_weights = co_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "co_weights = pd.concat([co_weights.head(5),co_weights.tail(5)])\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Counter Features\", data=co_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LQ1T4lSHrrR",
    "outputId": "cc6c3a2b-1225-481a-e63c-620df6eb8569"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if 'teach' in X_train[i]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz0ZyHagHrrS",
    "outputId": "e13f3288-d8d4-43ef-c69e-0bfcb5ff9735"
   },
   "outputs": [],
   "source": [
    "X_train[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1x0n1qnHrrT"
   },
   "outputs": [],
   "source": [
    "weights, real_prediction, local_prediction = lionet.explain_instance(x_train[119][0:], 2000, transparent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QEhMVc4HrrU",
    "outputId": "622ce882-3554-4681-a5b6-d04fc8d2088e"
   },
   "outputs": [],
   "source": [
    "model_weights = pd.DataFrame({\"Features\": list(vec.get_feature_names()), \n",
    "                                \"Features' Weights\": list(weights*x_train[119][0:])})\n",
    "model_weights = model_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "model_weights = model_weights[(model_weights[\"Features' Weights\"] != 0)]    \n",
    "#model_weights, lime_predict([text])[0][1], rd.predict(texts)[0], weights\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=model_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative using autodecoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_instance = x_train[9].copy()\n",
    "transparent_model = Ridge(alpha=0.0001,fit_intercept=True,random_state=0)\n",
    "weights, real_prediction, local_prediction = autolionet.explain_instance(temp_instance[0:],2000, transparent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str('Sentence: \"' + X_train[9] + '\"   Class: ' + str(train_y[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, real_prediction, local_prediction = autolionet.explain_instance(x_train[9][0:], 2000, transparent_model)\n",
    "print(\"Real prediction:\",real_prediction,\", Local prediction:\",local_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = pd.DataFrame({\"Features\": list(vec.get_feature_names()), \n",
    "                              \"Features' Weights\": list(weights*x_train[9][0:])})\n",
    "model_weights = model_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "model_weights = model_weights[(model_weights[\"Features' Weights\"] != 0)]    \n",
    "#model_weights, lime_predict([text])[0][1], rd.predict(texts)[0], weights\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=model_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(weights),np.argmin(weights),np.max(weights),np.min(weights),vec.get_feature_names()[np.argmax(weights)],vec.get_feature_names()[np.argmin(weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_weights = []\n",
    "counter_features = []\n",
    "for i in range(len(weights)):\n",
    "    if weights[i]!=0:\n",
    "        if vec.get_feature_names()[i] not in X_train[9]:\n",
    "            counter_weights.append(weights[i])\n",
    "            counter_features.append(vec.get_feature_names()[i])\n",
    "co_weights = pd.DataFrame({\"Counter Features\": list(counter_features), \n",
    "                                  \"Features' Weights\": list(counter_weights)})\n",
    "co_weights = co_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "co_weights = pd.concat([co_weights.head(5),co_weights.tail(5)])\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Counter Features\", data=co_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if 'teach' in X_train[i]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, real_prediction, local_prediction = autolionet.explain_instance(x_train[119][0:], 2000, transparent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = pd.DataFrame({\"Features\": list(vec.get_feature_names()), \n",
    "                              \"Features' Weights\": list(weights*x_train[119][0:])})\n",
    "model_weights = model_weights.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "model_weights = model_weights[(model_weights[\"Features' Weights\"] != 0)]    \n",
    "#model_weights, lime_predict([text])[0][1], rd.predict(texts)[0], weights\n",
    "plt.figure(num=None, figsize=(4, 3), dpi=200, facecolor='w', edgecolor='k')\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=model_weights)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(str('Features not appearing in the instance'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LioNets_SMSSpam_TFIDF_Thesis_Example.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
